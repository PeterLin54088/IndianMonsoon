{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "371a4a29",
   "metadata": {},
   "source": [
    "## NBconvertApp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f4b1b0b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "➡️ Converting: /home/b08209033/IndianMonsoon/src/python/notebooks/constants.ipynb\n",
      "➡️ Converting: /home/b08209033/IndianMonsoon/src/python/notebooks/utils.ipynb\n",
      "➡️ Converting: /home/b08209033/IndianMonsoon/src/python/notebooks/calculations.ipynb\n",
      "➡️ Converting: /home/b08209033/IndianMonsoon/src/python/notebooks/LinearShallowEquatorialWave.ipynb\n",
      "➡️ Converting: /home/b08209033/IndianMonsoon/src/python/notebooks/plotter.ipynb\n",
      "✅ All done, output saved in: /home/b08209033/IndianMonsoon/src/python/modules\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    import subprocess, os\n",
    "\n",
    "    subprocess.run([\"bash\", \"../convert.sh\"], check=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfdf28f7",
   "metadata": {},
   "source": [
    "## Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "512d4b40",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from netCDF4 import Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa29dd33",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e1aebe9",
   "metadata": {},
   "source": [
    "### Indian Monsoon Index (Bin Wang)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84ea1db0",
   "metadata": {},
   "source": [
    "**Explanation (conceptually)**\n",
    "\n",
    "1. Slicing : $u(15695, 8, 180 ,360,) \\rightarrow u_{A, B}(15695, 1, 18 ,60,) \\rightarrow u_{A, B}(15695, 18 ,60,)$\n",
    "2. Averaging : $u_{A, B}(15695, 18 ,60,) \\rightarrow u_{A, B}(15695,)$\n",
    "3. Subtract : $\\text{IMI} = u_{B}(15695,) - u_{A}(15695,)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb4eae30",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Indian_Monsoon_Onset_Bin_Wang(filepath: str) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Computes the Indian Monsoon Index (IMI) as the difference in zonal wind\n",
    "    between southern and northern regions defined by geographic masks, using\n",
    "    data from a netCDF file.\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    filepath : str\n",
    "        Path to the netCDF file containing zonal wind data.\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    np.ndarray\n",
    "        A 2D array of smoothed IMI values with shape (-1, 365), where each row\n",
    "        represents a year's daily values.\n",
    "    \"\"\"\n",
    "    from .constants import BinWang2008_MASK\n",
    "    from .utils import moving_average\n",
    "\n",
    "    with Dataset(filepath, mode=\"r\") as dataset:\n",
    "        varname = \"u\"\n",
    "        dimension_names = dataset[varname].dimensions\n",
    "        dimensions = {name: dataset[name][:] for name in dimension_names}\n",
    "\n",
    "        northern_slice = [slice(None)] * len(dimensions)\n",
    "        southern_slice = [slice(None)] * len(dimensions)\n",
    "        for idx, key in enumerate(dimension_names):\n",
    "            if key == \"time\":\n",
    "                continue\n",
    "            elif key == \"plev\":\n",
    "                plev_mask = np.argwhere(dimensions[key] == 85000)[-1]\n",
    "                dimensions[key] = dimensions[key][plev_mask] / 100\n",
    "                northern_slice[idx] = southern_slice[idx] = plev_mask\n",
    "            elif key == \"lat\":\n",
    "                northern_slice[idx] = (\n",
    "                    dimensions[\"lat\"] <= BinWang2008_MASK.NORTHERN_LATITUDE_NORTH\n",
    "                ) & (dimensions[\"lat\"] >= BinWang2008_MASK.NORTHERN_LATITUDE_SOUTH)\n",
    "                southern_slice[idx] = (\n",
    "                    dimensions[\"lat\"] <= BinWang2008_MASK.SOUTHERN_LATITUDE_NORTH\n",
    "                ) & (dimensions[\"lat\"] >= BinWang2008_MASK.SOUTHERN_LATITUDE_SOUTH)\n",
    "            elif key == \"lon\":\n",
    "                northern_slice[idx] = (\n",
    "                    dimensions[\"lon\"] <= BinWang2008_MASK.NORTHERN_LONGITUDE_EAST\n",
    "                ) & (dimensions[\"lon\"] >= BinWang2008_MASK.NORTHERN_LONGITUDE_WEST)\n",
    "                southern_slice[idx] = (\n",
    "                    dimensions[\"lon\"] <= BinWang2008_MASK.SOUTHERN_LONGITUDE_EAST\n",
    "                ) & (dimensions[\"lon\"] >= BinWang2008_MASK.SOUTHERN_LONGITUDE_WEST)\n",
    "        zonal_wind_north = (\n",
    "            dataset[varname][tuple(northern_slice)].squeeze().mean(axis=(1, 2))\n",
    "        )\n",
    "        zonal_wind_south = (\n",
    "            dataset[varname][tuple(southern_slice)].squeeze().mean(axis=(1, 2))\n",
    "        )\n",
    "    indian_monsoon_index = zonal_wind_south - zonal_wind_north\n",
    "    indian_monsoon_index_smoothed = moving_average(indian_monsoon_index).reshape(\n",
    "        -1, 365\n",
    "    )\n",
    "    return indian_monsoon_index_smoothed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89a9907a",
   "metadata": {},
   "source": [
    "### Vertical Mass Streamfunction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74ab722d",
   "metadata": {},
   "source": [
    "Reference from [here](https://derekyuntao.github.io/jekyll-clean-dark/2021/02/mass-stream-func/)\n",
    "\n",
    "**Explanation (conceptually)**\n",
    "\n",
    "1. Slicing : $v_D(15695, 8, 180 ,360,) \\rightarrow v_D(15695, 8, 20 ,72,)$\n",
    "2. Averaging : $v_D(15695, 8, 20 ,72,) \\rightarrow v_D(15695, 8, 20,)$\n",
    "3. Insert B.C. : $v_D(15695, 8, 20,) \\rightarrow v_D(15695, 9, 20,)$ and $p(8,) \\rightarrow p(9,)$\n",
    "4. Midpoint Averaging : $v_D(15695, 9, 20,) \\rightarrow v_D(15695, 8, 20,)$ and $p(9,) \\rightarrow \\Delta p(8,)$\n",
    "5. Multiply some weightings and perform cumulative sum.\n",
    "\n",
    "\n",
    "**Notes**\n",
    "\n",
    "To solve mass streamfunction numerically, we apply **midpoint Riemann sum approximation**:\n",
    "\n",
    "$$\n",
    "\\psi[...,n] := a \\cos \\phi \\Delta \\lambda \\frac{1}{g} \\sum_{i = 1}^{N = n} [\\bar{v}_D]_i [\\Delta p^{\\prime}]_i + \\psi[...,0], \\quad \\forall n \\in Z\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "$$\n",
    "[\\bar{v}_D]_i = \\frac{\\bar{v}_D [i-1] + \\bar{v}_D [i]}{2}\n",
    "$$\n",
    "\n",
    "$$\n",
    "[\\Delta p^{\\prime}]_i = p [i] - p [i-1]\n",
    "$$\n",
    "\n",
    "Ideally, we set $\\psi[...,0] = \\bar{v}_D[...,0] = 0$ as our boundary conditions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ed5cf7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def streamfunction_Schwendike(\n",
    "    filepath: str,\n",
    ") -> tuple[np.ndarray, dict[str, np.ndarray]]:\n",
    "    \"\"\"\n",
    "    Compute the mass streamfunction using the Schwendike et al. method.\n",
    "\n",
    "    Parameters:\n",
    "        filepath (str): Path to the dataset file containing meridional wind data.\n",
    "\n",
    "    Returns:\n",
    "        tuple[np.ndarray, dict[str, np.ndarray]]:\n",
    "            - Smoothed mass streamfunction (3D array).\n",
    "            - Dimensions dictionary containing pressure levels, latitude, and longitude arrays.\n",
    "    \"\"\"\n",
    "    from .utils import moving_average, split_dimension\n",
    "    from .constants import REGION_MASK\n",
    "    from .constants import EARTH_PARAMETER\n",
    "\n",
    "    with Dataset(filepath, mode=\"r\") as dataset:\n",
    "        varname = \"v\"\n",
    "        dimension_names = dataset[varname].dimensions\n",
    "        dimensions = {name: dataset[name][:] for name in dimension_names}\n",
    "\n",
    "        data_slice = [slice(None)] * len(dimensions)\n",
    "        for idx, key in enumerate(dimension_names):\n",
    "            if key == \"time\":\n",
    "                continue\n",
    "            if key == \"plev\":\n",
    "                data_slice[idx] = slice(None, None, -1)\n",
    "                dimensions[\"plev\"] = dimensions[\"plev\"][data_slice[idx]]\n",
    "            elif key == \"lat\":\n",
    "                data_slice[idx] = (dimensions[\"lat\"] <= REGION_MASK.LATITUDE_NORTH) & (\n",
    "                    dimensions[\"lat\"] >= REGION_MASK.LATITUDE_SOUTH\n",
    "                )\n",
    "                dimensions[\"lat\"] = dimensions[\"lat\"][data_slice[idx]]\n",
    "            elif key == \"lon\":\n",
    "                data_slice[idx] = (dimensions[\"lon\"] <= REGION_MASK.LONGITUDE_EAST) & (\n",
    "                    dimensions[\"lon\"] >= REGION_MASK.LONGITUDE_WEST\n",
    "                )\n",
    "                dimensions[\"lon\"] = dimensions[\"lon\"][data_slice[idx]]\n",
    "\n",
    "        divergent_meridional_wind = dataset[varname][tuple(data_slice)]\n",
    "\n",
    "    divergent_meridional_wind = np.insert(divergent_meridional_wind, 0, 0, axis=1)\n",
    "    pressure_levels = np.insert(dimensions[\"plev\"], 0, 0)\n",
    "    divergent_meridional_wind = np.mean(divergent_meridional_wind, axis=-1)\n",
    "    divergent_meridional_wind = (\n",
    "        divergent_meridional_wind[:, :-1, :] + divergent_meridional_wind[:, 1:, :]\n",
    "    ) / 2  # Riemann sum, middle point method (discrete integration)\n",
    "\n",
    "    longitudinal_extent = np.deg2rad(dimensions[\"lon\"][-1] - dimensions[\"lon\"][0])\n",
    "    latitudinal_weighting = np.cos(np.deg2rad(dimensions[\"lat\"]))\n",
    "    weighting_factor = (\n",
    "        (EARTH_PARAMETER.RADIUS / EARTH_PARAMETER.GRAVITY_ACCELERATION)\n",
    "        * longitudinal_extent\n",
    "        * latitudinal_weighting\n",
    "    )\n",
    "\n",
    "    streamfunction = np.swapaxes(divergent_meridional_wind, 1, -1) * np.diff(\n",
    "        pressure_levels\n",
    "    )\n",
    "    streamfunction = np.cumsum(streamfunction, axis=-1)\n",
    "    streamfunction = np.swapaxes(streamfunction, -1, 1) * weighting_factor\n",
    "\n",
    "    streamfunction_smoothed = moving_average(streamfunction, axis=0)\n",
    "    streamfunction_smoothed = split_dimension(streamfunction_smoothed, axis=0)\n",
    "    dimensions[\"plev\"] /= 100  # Convert pressure levels to hPa\n",
    "\n",
    "    return streamfunction_smoothed, dimensions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eb92dfd",
   "metadata": {},
   "source": [
    "### MSE_vertical_flux"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e26d37c",
   "metadata": {},
   "source": [
    "**Explanation (conceptually)**\n",
    "\n",
    "1. Slicing : $w(15695, 8, 180 ,360,) \\rightarrow w(15695, 8, 20 ,72,)$\n",
    "2. Slicing : $\\theta(15695, 8, 180 ,360,) \\rightarrow \\theta(15695, 8, 20 ,72,)$\n",
    "3. Pointwise Multiply : $ \\rightarrow w \\theta(15695, 8, 20 ,72,)$\n",
    "4. Averaging : $w \\theta(15695, 8, 20 ,72,) \\rightarrow w \\theta(15695, 8, 20,)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0dfbbeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_MSE_vertical_flux(\n",
    "    mse_filepath: str, w_filepath: str\n",
    ") -> tuple[np.ndarray, dict[str, np.ndarray]]:\n",
    "    \"\"\"\n",
    "    Calculate the mean and smoothed vertical flux of moist static energy (MSE)\n",
    "    within a specific geographical mask.\n",
    "\n",
    "    Args:\n",
    "        mse_filepath (str): Filepath to the NetCDF file containing moist static energy data.\n",
    "        w_filepath (str): Filepath to the NetCDF file containing vertical velocity data.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[np.ndarray, Dict[str, np.ndarray]]:\n",
    "            - Smoothed MSE vertical flux data as a NumPy array.\n",
    "            - Dimensions dictionary with processed coordinate arrays.\n",
    "    \"\"\"\n",
    "    from .utils import moving_average, split_dimension\n",
    "    from .constants import REGION_MASK\n",
    "\n",
    "    with Dataset(mse_filepath, mode=\"r\") as mse_dataset, Dataset(\n",
    "        w_filepath, mode=\"r\"\n",
    "    ) as w_dataset:\n",
    "        varname = \"mse\"\n",
    "        dimension_names = mse_dataset[varname].dimensions\n",
    "        dimensions = {name: mse_dataset[name][:] for name in dimension_names}\n",
    "\n",
    "        data_slice = [slice(None)] * len(dimensions)\n",
    "        for idx, key in enumerate(mse_dataset[varname].dimensions):\n",
    "            if key == \"time\":\n",
    "                continue\n",
    "            elif key == \"plev\":\n",
    "                continue\n",
    "            elif key == \"lat\":\n",
    "                data_slice[idx] = (dimensions[\"lat\"] <= REGION_MASK.LATITUDE_NORTH) & (\n",
    "                    dimensions[\"lat\"] >= REGION_MASK.LATITUDE_SOUTH\n",
    "                )\n",
    "                dimensions[\"lat\"] = dimensions[\"lat\"][data_slice[idx]]\n",
    "            elif key == \"lon\":\n",
    "                data_slice[idx] = (dimensions[\"lon\"] <= REGION_MASK.LONGITUDE_EAST) & (\n",
    "                    dimensions[\"lon\"] >= REGION_MASK.LONGITUDE_WEST\n",
    "                )\n",
    "                dimensions[\"lon\"] = dimensions[\"lon\"][data_slice[idx]]\n",
    "        moist_static_energy = mse_dataset[\"mse\"][tuple(data_slice)]\n",
    "        pressure_tendency = w_dataset[\"w\"][tuple(data_slice)]\n",
    "\n",
    "    moist_static_energy_flux = np.mean(moist_static_energy * pressure_tendency, axis=-1)\n",
    "    moist_static_energy_flux_smoothed = moving_average(moist_static_energy_flux, axis=0)\n",
    "    moist_static_energy_flux_smoothed = split_dimension(\n",
    "        moist_static_energy_flux_smoothed, axis=0\n",
    "    )\n",
    "\n",
    "    dimensions[\"plev\"] /= 100\n",
    "    return moist_static_energy_flux_smoothed, dimensions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edc30d6c",
   "metadata": {},
   "source": [
    "### Wheeler Kiladis Diagram"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "146106cf",
   "metadata": {},
   "source": [
    "**Explanation (conceptually)**\n",
    "\n",
    "0. Choose pressure level (implemented outside of method) : $\\lambda(15695, 8, 180, 360,) \\rightarrow \\lambda(15695, 180, 360,)$\n",
    "1. Detrend : $\\lambda(15695, 180, 360,) \\rightarrow \\lambda(15695, 180, 360,)$\n",
    "2. LPF : $\\lambda(15695, 180, 360,) \\rightarrow \\lambda(15695, 180, 360,)$\n",
    "3. Sym & AntiSym decomposition : $\\lambda(15695, 180, 360,) \\rightarrow \\lambda_{\\text{Sym}, \\text{Asym}}(2, 15695, 180, 360,)$\n",
    "4. Slicing : $\\lambda_{\\text{Sym}, \\text{Asym}}(2, 15695, 180, 360,) \\rightarrow \\lambda_{\\text{Sym}, \\text{Asym}}(2, 15695, 25, 360,)$\n",
    "5. Segmentation (TODO) : $\\lambda_{\\text{Sym}, \\text{Asym}}(2, 15695, 25, 360,) \\rightarrow \\lambda_{\\text{Sym}, \\text{Asym}}(2, 15695 // seg, seg, 25, 360,)$, seg need to be assigned.\n",
    "\n",
    "Since we operate on each segment, consider \n",
    "\n",
    "5. $\\lambda_{\\text{Sym}, \\text{Asym}}(2, seg, 25, 360,)$\n",
    "\n",
    "6. Detrend : $\\lambda_{\\text{Sym}, \\text{Asym}}(2, seg, 25, 360,) \\rightarrow \\lambda_{\\text{Sym}, \\text{Asym}}(2, seg, 25, 360,)$\n",
    "\n",
    "7. Tapering : $\\lambda_{\\text{Sym}, \\text{Asym}}(2, seg, 25, 360,) \\rightarrow \\lambda_{\\text{Sym}, \\text{Asym}}(2, seg, 25, 360,)$\n",
    "\n",
    "8. FFT : $\\lambda_{\\text{Sym}, \\text{Asym}}(2, seg, 25, 360,) \\rightarrow \\lambda_{\\text{Sym}, \\text{Asym}}(2, seg, 25, 360,)$\n",
    "\n",
    "9. Powerspec : $\\lambda_{\\text{Sym}, \\text{Asym}}(2, seg, 25, 360,) \\rightarrow \\lambda_{\\text{Sym}, \\text{Asym}}(2, seg, 25, 360,)$\n",
    "\n",
    "10. Averaging : $\\lambda_{\\text{Sym}, \\text{Asym}}(2, seg, 25, 360,) \\rightarrow \\lambda_{\\text{Sym}, \\text{Asym}}(2, seg, 360,)$\n",
    "\n",
    "11. Smoothing : $\\lambda_{\\text{Sym}, \\text{Asym}}(2, seg, 25, 360,) \\rightarrow \\lambda_{\\text{Sym}, \\text{Asym}}(2, seg, 360,)$\n",
    "\n",
    "12. Divide : $\\lambda_{\\text{Sym}, \\text{Asym}}(2, seg, 25, 360,) \\rightarrow \\lambda_{\\text{Sym}, \\text{Asym}}(2, seg, 360,)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91605f07",
   "metadata": {},
   "outputs": [],
   "source": [
    "def power_spectrum_Wheeler_Kiladis(\n",
    "    variable: np.ndarray, dimensions: dict[str, np.ndarray], **kwargs\n",
    ") -> tuple[np.ndarray, np.ndarray, np.ndarray, dict[str, np.ndarray]]:\n",
    "    from scipy.signal import detrend as scipy_linear_detrend\n",
    "    import time\n",
    "\n",
    "    def __initialize_kwargs(kwargs: dict) -> dict:\n",
    "        \"\"\"\n",
    "        Initializes the provided `kwargs` dictionary with default values.\n",
    "\n",
    "        Parameters:\n",
    "            kwargs (dict): A dictionary of optional parameters to initialize or update.\n",
    "\n",
    "        Returns:\n",
    "            dict: The updated dictionary containing default values for missing keys.\n",
    "        \"\"\"\n",
    "        from .constants import REGION_MASK, WKD_PARAMETER\n",
    "\n",
    "        boundary_defaults = {\n",
    "            \"east_boundary\": REGION_MASK.LONGITUDE_EAST,\n",
    "            \"west_boundary\": REGION_MASK.LONGITUDE_WEST,\n",
    "            \"north_boundary\": REGION_MASK.LATITUDE_NORTH,\n",
    "            \"south_boundary\": REGION_MASK.LATITUDE_SOUTH,\n",
    "        }\n",
    "\n",
    "        segmentation_defaults = {\n",
    "            \"segment_length\": WKD_PARAMETER.SEGMENTATION_LENGTH,\n",
    "            \"overlap_length\": WKD_PARAMETER.OVERLAP_LENGTH,\n",
    "        }\n",
    "\n",
    "        convolution_defaults = {\"kernel\": np.array([1 / 4, 1 / 2, 1 / 4])}\n",
    "        convolution_defaults.update(\n",
    "            {\n",
    "                \"smoother\": lambda m: np.convolve(\n",
    "                    m, convolution_defaults[\"kernel\"], mode=\"valid\"\n",
    "                )\n",
    "            }\n",
    "        )\n",
    "\n",
    "        other_defaults = {\n",
    "            \"cutoff_frequency\": 1 / WKD_PARAMETER.SEGMENTATION_LENGTH,\n",
    "            \"sampling_rate\": 1,\n",
    "        }\n",
    "\n",
    "        defaults = {}\n",
    "        defaults.update(boundary_defaults)\n",
    "        defaults.update(segmentation_defaults)\n",
    "        defaults.update(convolution_defaults)\n",
    "        defaults.update(other_defaults)\n",
    "\n",
    "        for key, value in defaults.items():\n",
    "            kwargs.setdefault(key, value)\n",
    "\n",
    "        return kwargs\n",
    "\n",
    "    kwargs = __initialize_kwargs(kwargs)\n",
    "\n",
    "    def __high_pass_filter(\n",
    "        signal: np.ndarray,\n",
    "        axis: int,\n",
    "        cutoff_frequency: float = kwargs[\"cutoff_frequency\"],\n",
    "    ) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Applies a high-pass filter to the input signal by removing frequency components\n",
    "        below the specified cutoff frequency.\n",
    "\n",
    "        Parameters:\n",
    "            signal (np.ndarray): The input signal array to filter.\n",
    "            axis (int): The axis along which to apply the Fourier transform and filtering.\n",
    "            cutoff_frequency (float): The cutoff frequency for the high-pass filter.\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: The filtered signal after removing low-frequency components.\n",
    "        \"\"\"\n",
    "        signal_length = signal.shape[axis]\n",
    "\n",
    "        fourier_component = np.fft.rfft(signal, axis=axis)\n",
    "        positive_frequencies = np.fft.rfftfreq(signal_length)\n",
    "\n",
    "        filter_condition = [slice(None)] * signal.ndim\n",
    "        filter_condition[axis] = positive_frequencies < cutoff_frequency\n",
    "        fourier_component[tuple(filter_condition)] = 0.0\n",
    "\n",
    "        filtered_signal = np.fft.irfft(fourier_component, n=signal_length, axis=axis)\n",
    "\n",
    "        return filtered_signal\n",
    "\n",
    "    def __decompose_symmetric_antisymmetric(\n",
    "        variable: np.ndarray, axis: int\n",
    "    ) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Decomposes the input array into its symmetric and antisymmetric components\n",
    "        along equator.\n",
    "\n",
    "        Parameters:\n",
    "            variable (np.ndarray): The input array to decompose.\n",
    "            axis (int): The axis along which the decomposition is performed, should be latitude.\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: A two-element array containing the symmetric component\n",
    "            at index 0 and the antisymmetric component at index 1.\n",
    "        \"\"\"\n",
    "        flipped_variable = np.flip(variable, axis=axis)\n",
    "        symmetric_component = (variable + flipped_variable) / 2\n",
    "        antisymmetric_component = (variable - flipped_variable) / 2\n",
    "\n",
    "        return np.array([symmetric_component, antisymmetric_component])\n",
    "\n",
    "    def __latitude_masking(\n",
    "        variable: np.ndarray, dimensions: dict[str, np.ndarray], axis: int\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Applies a latitude mask to filter a variable and its corresponding latitude dimension.\n",
    "\n",
    "        Parameters:\n",
    "            variable (np.ndarray): The input array to be masked.\n",
    "            dimensions (dict[str, np.ndarray]): A dictionary containing dimension arrays,\n",
    "                including \"lat\" for latitude.\n",
    "            axis (int): The axis in the `variable` array corresponding to the latitude dimension.\n",
    "\n",
    "        Returns:\n",
    "            tuple: A tuple containing:\n",
    "                - np.ndarray: The masked variable array.\n",
    "                - dict[str, np.ndarray]: The updated dimensions dictionary with the masked \"lat\".\n",
    "        \"\"\"\n",
    "        data_slice = [slice(None)] * variable.ndim\n",
    "        lat_mask = (dimensions[\"lat\"] <= kwargs[\"north_boundary\"]) & (\n",
    "            dimensions[\"lat\"] >= kwargs[\"south_boundary\"]\n",
    "        )\n",
    "        data_slice[axis] = lat_mask\n",
    "        variable = variable[tuple(data_slice)]\n",
    "        dimensions[\"lat\"] = dimensions[\"lat\"][lat_mask]\n",
    "        return variable, dimensions\n",
    "\n",
    "    def __smoothing_filter(\n",
    "        signal: np.ndarray,\n",
    "        axis: int,\n",
    "        iterations: int,\n",
    "    ) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Applies a smoothing filter to the input signal along a specified axis for a given\n",
    "        number of iterations.\n",
    "\n",
    "        Parameters:\n",
    "            signal (np.ndarray): The input signal array to be smoothed.\n",
    "            axis (int): The axis along which the smoothing filter is applied.\n",
    "            iterations (int): The number of smoothing iterations to perform.\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: The smoothed signal array.\n",
    "        \"\"\"\n",
    "\n",
    "        def __duplicate_boundaries(signal: np.ndarray, axis: int) -> np.ndarray:\n",
    "            boundary_slice = [slice(None)] * signal.ndim\n",
    "            boundary_slice[axis] = slice(None, 1)\n",
    "            left_boundary = signal[tuple(boundary_slice)]\n",
    "\n",
    "            boundary_slice = [slice(None)] * signal.ndim\n",
    "            boundary_slice[axis] = slice(-1, None)\n",
    "            right_boundary = signal[tuple(boundary_slice)]\n",
    "\n",
    "            return np.concatenate(\n",
    "                (left_boundary, signal, right_boundary),\n",
    "                axis=axis,\n",
    "            )\n",
    "\n",
    "        def __Gaussian_blur(signal: np.ndarray, axis: int):\n",
    "            return np.apply_along_axis(\n",
    "                kwargs[\"smoother\"],\n",
    "                axis=axis,\n",
    "                arr=signal,\n",
    "            )\n",
    "\n",
    "        for _ in range(iterations):\n",
    "            signal = __duplicate_boundaries(signal, axis=axis)\n",
    "            signal = __Gaussian_blur(signal, axis=axis)\n",
    "        return signal\n",
    "\n",
    "    def __temporal_taper(signal: np.ndarray, portion: float, axis: int) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Applies a temporal tapering window to the signal along the specified axis.\n",
    "\n",
    "        Parameters:\n",
    "            signal (np.ndarray): The input signal array to which the taper will be applied.\n",
    "            portion (float): The fraction of the signal's length (per axis) to apply the taper.\n",
    "                Must be between 0 and 1.\n",
    "            axis (int): The axis along which the taper is applied.\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: A tapering window that can be applied to the signal.\n",
    "        \"\"\"\n",
    "\n",
    "        def __get_taper():\n",
    "            taper_width = int(signal.shape[axis] * (portion / 2))\n",
    "            taper = 0.5 * (\n",
    "                1 - np.cos(2 * np.pi * np.arange(taper_width) / (2 * taper_width))\n",
    "            )\n",
    "\n",
    "            taper_window = np.ones(signal.shape[axis])\n",
    "            taper_window[:taper_width] = taper\n",
    "            taper_window[-taper_width:] = taper[::-1]\n",
    "\n",
    "            broadcast_shape = [1] * signal.ndim\n",
    "            broadcast_shape[axis] = signal.shape[axis]\n",
    "            taper_window = taper_window.reshape(broadcast_shape)\n",
    "            return taper_window\n",
    "\n",
    "        taper = __get_taper()\n",
    "\n",
    "        return taper\n",
    "\n",
    "    def __spatial_taper(\n",
    "        signal: np.ndarray,\n",
    "        dimensions: dict[str, np.ndarray],\n",
    "        axis: int,\n",
    "    ) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Applies a spatial tapering window to the signal along a longitude axis.\n",
    "\n",
    "        Parameters:\n",
    "            signal (np.ndarray): The input signal array to which the taper will be applied.\n",
    "            dimensions (dict[str, np.ndarray]): A dictionary containing dimension arrays,\n",
    "                including \"lon\" for longitude.\n",
    "            axis (int): The axis corresponding to the longitude dimension in the signal.\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: A spatial tapering window that can be applied to the signal.\n",
    "        \"\"\"\n",
    "\n",
    "        def __get_taper():\n",
    "            longitude = dimensions[\"lon\"]\n",
    "            global_flag = (\n",
    "                int(abs(kwargs[\"east_boundary\"] - kwargs[\"west_boundary\"])) == 360\n",
    "            )\n",
    "\n",
    "            if global_flag:\n",
    "                taper_window = np.ones_like(longitude)\n",
    "            else:\n",
    "                taper_window = np.zeros_like(longitude, dtype=float)\n",
    "                regional_mask = (longitude >= kwargs[\"west_boundary\"]) & (\n",
    "                    longitude <= kwargs[\"east_boundary\"]\n",
    "                )\n",
    "                regional_taper_window = taper_window[regional_mask]\n",
    "                taper_width = int(len(regional_taper_window) * 0.1)\n",
    "                taper = 0.5 * (\n",
    "                    1 - np.cos(2 * np.pi * np.arange(taper_width) / (2 * taper_width))\n",
    "                )\n",
    "                regional_taper_window[:taper_width] = taper\n",
    "                regional_taper_window[taper_width:-taper_width] = 1\n",
    "                regional_taper_window[-taper_width:] = taper[::-1]\n",
    "                taper_window[regional_mask] = regional_taper_window\n",
    "\n",
    "            broadcast_shape = [1] * signal.ndim\n",
    "            broadcast_shape[axis] = len(taper_window)\n",
    "            taper_window = taper_window.reshape(broadcast_shape)\n",
    "            return taper_window\n",
    "\n",
    "        taper = __get_taper()\n",
    "        return taper\n",
    "\n",
    "    def __process_segment(\n",
    "        variable: np.ndarray,\n",
    "        dimensions: dict[str, np.ndarray],\n",
    "        axis: int,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Processes a multidimensional array by dividing it into segments, applying tapers,\n",
    "        detrending, and performing spectral analysis.\n",
    "\n",
    "        Parameters:\n",
    "            variable (np.ndarray): The input data array to be processed.\n",
    "            dimensions (dict[str, np.ndarray]): Dictionary of dimension arrays, such as \"lon\" for longitude.\n",
    "            axis (int): The axis along which the segmentation and processing are applied.\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: The computed symmetric-antisymmetric power spectrum.\n",
    "        \"\"\"\n",
    "\n",
    "        def __segment_data(\n",
    "            data: np.ndarray,\n",
    "            axis: int,\n",
    "        ):\n",
    "            step = kwargs[\"segment_length\"] - kwargs[\"overlap_length\"]\n",
    "            num_iterations = (data.shape[axis] - kwargs[\"overlap_length\"]) // step\n",
    "\n",
    "            def __generator():\n",
    "                for counter in range(num_iterations):\n",
    "                    start = counter * step\n",
    "                    end = start + kwargs[\"segment_length\"]\n",
    "                    segment_slice = [\n",
    "                        slice(None)\n",
    "                    ] * data.ndim  # Create slices for all dimensions\n",
    "                    segment_slice[axis] = slice(\n",
    "                        start, end\n",
    "                    )  # Set slice for the target axis\n",
    "                    yield data[tuple(segment_slice)]\n",
    "\n",
    "            return num_iterations, __generator()\n",
    "\n",
    "        powerspec_shape = list(np.shape(variable))\n",
    "        del powerspec_shape[-2]\n",
    "        powerspec_shape[axis] = kwargs[\"segment_length\"]\n",
    "        sym_asym_powerspec = np.zeros(shape=tuple(powerspec_shape), dtype=float)\n",
    "\n",
    "        __segment_data = kwargs.get(\"segment_method\", __segment_data)\n",
    "        num_segments, segment_iterator = __segment_data(\n",
    "            data=variable,\n",
    "            axis=axis,\n",
    "        )\n",
    "\n",
    "        for idx, segment in enumerate(segment_iterator):\n",
    "            if idx == 0:\n",
    "                temporal_taper = __temporal_taper(segment, portion=0.2, axis=axis)\n",
    "                spatial_taper = __spatial_taper(segment, dimensions=dimensions, axis=-1)\n",
    "\n",
    "            segment = scipy_linear_detrend(segment, axis=axis)\n",
    "            segment *= temporal_taper\n",
    "            segment *= spatial_taper\n",
    "            segment = np.fft.fft(segment, axis=-1, norm=\"ortho\")\n",
    "            segment = np.fft.ifft(segment, axis=1, norm=\"ortho\")\n",
    "\n",
    "            segment = np.sum(np.abs(segment) ** 2, axis=-2)\n",
    "            sym_asym_powerspec += segment\n",
    "\n",
    "        sym_asym_powerspec /= num_segments\n",
    "        sym_asym_powerspec = np.fft.fftshift(sym_asym_powerspec, axes=(axis, -1))\n",
    "\n",
    "        return sym_asym_powerspec\n",
    "\n",
    "    def __update_dimensions(dimensions: dict[str, np.ndarray]):\n",
    "        \"\"\"\n",
    "        Updates the dimensions dictionary by calculating and adding zonal wavenumbers\n",
    "        and segment frequencies in cycles per day (CPD).\n",
    "\n",
    "        Parameters:\n",
    "            dimensions (dict[str, np.ndarray]): A dictionary containing dimension arrays,\n",
    "                including \"lon\" for longitude.\n",
    "\n",
    "        Returns:\n",
    "            dict[str, np.ndarray]: The updated dimensions dictionary with added keys:\n",
    "                - \"segment_frequency\": Segment frequencies in CPD.\n",
    "                - \"zonal_wavenumber\": Wavenumbers corresponding to the longitude dimension.\n",
    "        \"\"\"\n",
    "        ordinary_wavenumber = np.fft.fftshift(\n",
    "            np.fft.fftfreq(len(dimensions[\"lon\"]), 1 / len(dimensions[\"lon\"]))\n",
    "        )\n",
    "        ordinary_frequency = np.fft.fftshift(\n",
    "            np.fft.fftfreq(kwargs[\"segment_length\"], 1 / kwargs[\"segment_length\"])\n",
    "        )\n",
    "        CPD_frequency = (\n",
    "            ordinary_frequency / len(ordinary_frequency) * kwargs[\"sampling_rate\"]\n",
    "        )  # Convert frequency to cycles per day (CPD)\n",
    "        dimensions.update(\n",
    "            {\n",
    "                \"segment_frequency\": CPD_frequency,\n",
    "                \"zonal_wavenumber\": ordinary_wavenumber,\n",
    "            }\n",
    "        )\n",
    "        return dimensions\n",
    "\n",
    "    def __process_powerspec(\n",
    "        powerspec: np.ndarray, dimensions: dict[str, np.ndarray], axis: int\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Processes the power spectrum by applying smoothing filters to positive and negative\n",
    "        frequency components and setting zero-frequency components to NaN.\n",
    "\n",
    "        Parameters:\n",
    "            powerspec (np.ndarray): The input power spectrum to be processed.\n",
    "            dimensions (dict[str, np.ndarray]): A dictionary containing dimension arrays,\n",
    "                including \"segment_frequency\" for frequency values.\n",
    "            axis (int): The axis corresponding to the frequency dimension in the power spectrum.\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: The processed power spectrum with smoothed positive and negative\n",
    "            frequencies and zero-frequency components set to NaN.\n",
    "        \"\"\"\n",
    "        positive_data_slice = [slice(None)] * powerspec.ndim\n",
    "        positive_data_slice[axis] = dimensions[\"segment_frequency\"] > 0\n",
    "        negative_data_slice = [slice(None)] * powerspec.ndim\n",
    "        negative_data_slice[axis] = dimensions[\"segment_frequency\"] < 0\n",
    "        zero_data_slice = [slice(None)] * powerspec.ndim\n",
    "        zero_data_slice[axis] = dimensions[\"segment_frequency\"] == 0\n",
    "\n",
    "        powerspec[tuple(positive_data_slice)] = __smoothing_filter(\n",
    "            powerspec[tuple(positive_data_slice)], axis=axis, iterations=1\n",
    "        )\n",
    "        powerspec[tuple(negative_data_slice)] = __smoothing_filter(\n",
    "            powerspec[tuple(negative_data_slice)], axis=axis, iterations=1\n",
    "        )\n",
    "        powerspec[tuple(zero_data_slice)] = np.nan\n",
    "\n",
    "        return powerspec\n",
    "\n",
    "    def __process_background_powerspec(\n",
    "        powerspec: np.ndarray, dimensions: dict[str, np.ndarray], axis: int\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Processes the background power spectrum by applying frequency-dependent smoothing filters\n",
    "        and handling positive, negative, and zero-frequency components.\n",
    "\n",
    "        Parameters:\n",
    "            powerspec (np.ndarray): The input background power spectrum to be processed.\n",
    "            dimensions (dict[str, np.ndarray]): A dictionary containing dimension arrays,\n",
    "                including \"segment_frequency\" for frequency values.\n",
    "            axis (int): The axis corresponding to the frequency dimension in the power spectrum.\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: The processed background power spectrum.\n",
    "        \"\"\"\n",
    "        powerspec = np.mean(powerspec, axis=axis)\n",
    "        for i, freq in enumerate(dimensions[\"segment_frequency\"]):\n",
    "            data_slice = [slice(None)] * powerspec.ndim\n",
    "            data_slice[axis] = i\n",
    "            if abs(freq) == 0.0:\n",
    "                continue\n",
    "            elif abs(freq) <= 0.1:\n",
    "                powerspec[tuple(data_slice)] = __smoothing_filter(\n",
    "                    powerspec[tuple(data_slice)], axis=axis, iterations=5\n",
    "                )\n",
    "            elif abs(freq) <= 0.2:\n",
    "                powerspec[tuple(data_slice)] = __smoothing_filter(\n",
    "                    powerspec[tuple(data_slice)], axis=axis, iterations=10\n",
    "                )\n",
    "            elif abs(freq) <= 0.3:\n",
    "                powerspec[tuple(data_slice)] = __smoothing_filter(\n",
    "                    powerspec[tuple(data_slice)], axis=axis, iterations=20\n",
    "                )\n",
    "            else:\n",
    "                powerspec[tuple(data_slice)] = __smoothing_filter(\n",
    "                    powerspec[tuple(data_slice)], axis=axis, iterations=40\n",
    "                )\n",
    "        positive_data_slice = [slice(None)] * powerspec.ndim\n",
    "        positive_data_slice[axis] = dimensions[\"segment_frequency\"] > 0\n",
    "        negative_data_slice = [slice(None)] * powerspec.ndim\n",
    "        negative_data_slice[axis] = dimensions[\"segment_frequency\"] < 0\n",
    "        zero_data_slice = [slice(None)] * powerspec.ndim\n",
    "        zero_data_slice[axis] = dimensions[\"segment_frequency\"] == 0\n",
    "\n",
    "        powerspec[tuple(positive_data_slice)] = __smoothing_filter(\n",
    "            powerspec[tuple(positive_data_slice)], axis=axis, iterations=10\n",
    "        )\n",
    "        powerspec[tuple(negative_data_slice)] = __smoothing_filter(\n",
    "            powerspec[tuple(negative_data_slice)], axis=axis, iterations=10\n",
    "        )\n",
    "        powerspec[tuple(zero_data_slice)] = np.nan\n",
    "        return powerspec\n",
    "\n",
    "    variable = scipy_linear_detrend(variable, axis=0)\n",
    "    variable = __high_pass_filter(variable, axis=0)\n",
    "    variable = __decompose_symmetric_antisymmetric(variable, axis=1)\n",
    "    variable, dimensions = __latitude_masking(variable, dimensions, axis=2)\n",
    "    powerspec = __process_segment(variable, dimensions, axis=1)\n",
    "    dimensions = __update_dimensions(dimensions)\n",
    "    sym_asym_powerspec = __process_powerspec(powerspec, dimensions, axis=1)\n",
    "    background_powerspec = __process_background_powerspec(powerspec, dimensions, axis=0)\n",
    "\n",
    "    return (\n",
    "        sym_asym_powerspec[0],\n",
    "        sym_asym_powerspec[1],\n",
    "        background_powerspec,\n",
    "        dimensions,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04091be6",
   "metadata": {},
   "source": [
    "### Wave Filtering (Legacy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41c9cc47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def calculate_filtered_signal(\n",
    "#     file_path: str,\n",
    "#     zonal_wavenumber_limit: np.ndarray,\n",
    "#     segmentation_frequency_limit: np.ndarray,\n",
    "#     variable_name: str = \"Undefined\",\n",
    "#     pressure_level: int = -1,\n",
    "# ) -> tuple[np.ndarray, np.ndarray, dict[str, np.ndarray]]:\n",
    "#     \"\"\"\n",
    "#     Filter out signal by decomposing into symmetric and antisymmetric components, FFT and mask out coefficient\n",
    "#     , and inverse FFT to derive filtered signal.\n",
    "\n",
    "#     Parameters:\n",
    "#     - file_path (str): Path to the NetCDF dataset.\n",
    "#     - zonal_wavenumber_limit (np.ndarray): Bounds for the zonal wavenumber filter (min, max).\n",
    "#     - segmentation_frequency_limit (np.ndarray): Bounds for frequency filter (min, max).\n",
    "#     - variable_name (str): Name of the variable to process in the dataset.\n",
    "#     - pressure_level (int): Pressure level index to slice data on if it contains a \"plev\" dimension.\n",
    "\n",
    "#     Returns:\n",
    "#         tuple: A tuple containing:\n",
    "#             - Symmetric components of the PSD.\n",
    "#             - Antisymmetric components of the PSD.\n",
    "#             - Dictionary of relevant dimensions.\n",
    "#     \"\"\"\n",
    "#     from gc import collect as free_memory\n",
    "#     from .utils import decompose_symmetric_antisymmetric\n",
    "#     from .constants import INDIAN_MASK\n",
    "\n",
    "#     # Load data and dimensions from the dataset\n",
    "#     with Dataset(file_path) as dataset:\n",
    "#         dims = {dim: dataset[dim][:] for dim in dataset[variable_name].dimensions}\n",
    "#         data_slices = [slice(None)] * len(dataset[variable_name].dimensions)\n",
    "\n",
    "#         for idx, dim in enumerate(dataset[variable_name].dimensions):\n",
    "#             if dim == \"time\" or dim == \"lat\" or dim == \"lon\":\n",
    "#                 continue\n",
    "#             elif dim == \"plev\":\n",
    "#                 data_slices[idx] = pressure_level\n",
    "#                 dims[dim] = dims[dim][data_slices[idx]]\n",
    "\n",
    "#         # Extract the data for the variable using the slices\n",
    "#         data = dataset[variable_name][tuple(data_slices)]\n",
    "\n",
    "#     # Mask latitudes based on geographic boundaries defined in INDIAN_MASK\n",
    "#     lat_mask = (dims[\"lat\"] <= INDIAN_MASK.LATITUDE_NORTH) & (\n",
    "#         dims[\"lat\"] >= INDIAN_MASK.LATITUDE_SOUTH\n",
    "#     )\n",
    "\n",
    "#     # Decompose data into symmetric and antisymmetric components\n",
    "#     symmetric_components, antisymmetric_components = decompose_symmetric_antisymmetric(\n",
    "#         data, axis=1\n",
    "#     )\n",
    "#     del data\n",
    "#     free_memory()\n",
    "\n",
    "#     # Mask the symmetric and antisymmetric components by latitude\n",
    "#     symmetric_components = symmetric_components[:, lat_mask, :]\n",
    "#     antisymmetric_components = antisymmetric_components[:, lat_mask, :]\n",
    "#     dims[\"lat\"] = dims[\"lat\"][lat_mask]\n",
    "\n",
    "#     # Perform FFT in both wavenumber (lon) and frequency (time) directions\n",
    "#     symmetric_components = np.fft.fft(symmetric_components, axis=-1, norm=\"ortho\")\n",
    "#     symmetric_components = np.fft.ifft(symmetric_components, axis=0, norm=\"ortho\")\n",
    "#     antisymmetric_components = np.fft.fft(\n",
    "#         antisymmetric_components, axis=-1, norm=\"ortho\"\n",
    "#     )\n",
    "#     antisymmetric_components = np.fft.ifft(\n",
    "#         antisymmetric_components, axis=0, norm=\"ortho\"\n",
    "#     )\n",
    "\n",
    "#     # Compute the wavenumber and frequency\n",
    "#     ordinary_wavenumber = np.fft.fftfreq(\n",
    "#         symmetric_components.shape[-1], 1 / symmetric_components.shape[-1]\n",
    "#     )\n",
    "#     CPD_frequency = (\n",
    "#         np.fft.fftfreq(symmetric_components.shape[0], 1 / symmetric_components.shape[0])\n",
    "#         / symmetric_components.shape[0]\n",
    "#     )\n",
    "\n",
    "#     # Create a filter mask based on zonal wavenumber and frequency limits\n",
    "#     mask = np.zeros_like(symmetric_components, dtype=bool)\n",
    "\n",
    "#     # Positive wavenumber and frequency filtering\n",
    "#     t_mask, y_mask, x_mask = np.meshgrid(\n",
    "#         (\n",
    "#             (CPD_frequency >= segmentation_frequency_limit[0])\n",
    "#             & (CPD_frequency <= segmentation_frequency_limit[1])\n",
    "#         ),\n",
    "#         np.ones(len(dims[\"lat\"]), dtype=bool),\n",
    "#         (ordinary_wavenumber >= zonal_wavenumber_limit[0])\n",
    "#         & (ordinary_wavenumber <= zonal_wavenumber_limit[1]),\n",
    "#         indexing=\"ij\",\n",
    "#     )\n",
    "#     mask = np.logical_or(mask, (t_mask & x_mask))\n",
    "\n",
    "#     # Negative wavenumber and frequency filtering\n",
    "#     t_mask, y_mask, x_mask = np.meshgrid(\n",
    "#         (\n",
    "#             (CPD_frequency >= -segmentation_frequency_limit[1])\n",
    "#             & (CPD_frequency <= -segmentation_frequency_limit[0])\n",
    "#         ),\n",
    "#         np.ones(len(dims[\"lat\"]), dtype=bool),\n",
    "#         (ordinary_wavenumber >= -zonal_wavenumber_limit[1])\n",
    "#         & (ordinary_wavenumber <= -zonal_wavenumber_limit[0]),\n",
    "#         indexing=\"ij\",\n",
    "#     )\n",
    "#     mask = np.logical_or(mask, (t_mask & x_mask))\n",
    "\n",
    "#     # Apply mask to the symmetric and antisymmetric components\n",
    "#     symmetric_components *= mask\n",
    "#     antisymmetric_components *= mask\n",
    "\n",
    "#     # Perform inverse FFT to bring the data back to the spatial and time domain\n",
    "#     symmetric_components = np.fft.fft(symmetric_components, axis=0, norm=\"ortho\")\n",
    "#     symmetric_components = np.fft.ifft(symmetric_components, axis=-1, norm=\"ortho\")\n",
    "#     antisymmetric_components = np.fft.fft(\n",
    "#         antisymmetric_components, axis=0, norm=\"ortho\"\n",
    "#     )\n",
    "#     antisymmetric_components = np.fft.ifft(\n",
    "#         antisymmetric_components, axis=-1, norm=\"ortho\"\n",
    "#     )\n",
    "#     return symmetric_components, antisymmetric_components, dims"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "__MAIN__",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
